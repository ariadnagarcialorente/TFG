# -*- coding: utf-8 -*-
"""FINAL_DATASET_ANALYSIS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QdRQVuWw1Gerze2RxUl5zshJRmHgpWLY

<h2> DATA COLLECTION <h2>
"""

# Uncomment if you are using google colab
#!pip install stats

# Uncomment if you are using google colab
# !pip install statsmodels

import pandas as pd
import numpy as np
import seaborn as sns
import scipy.stats as stats
import matplotlib.pyplot as plt
import statistics

from pyclustering.cluster.silhouette import silhouette_ksearch
from scipy.stats import shapiro, mannwhitneyu, kruskal, chi2_contingency, spearmanr
from decimal import Decimal
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.metrics import silhouette_score, silhouette_samples

filename = input("Enter the CSV filename (default: dataset_project_eHealth20242025.csv): ") or "dataset_project_eHealth20242025.csv"
df = pd.read_csv(filename)
df.head()

df.info()

df.shape[0] # Number of rows

df.shape[1] # Number of columns

"""<h4>Initially, the dataset has 160 rows and 93 columns.</h4>

<h2> DATA CLEANING <h2>

<h3> Processing rows <h3>

Duplicate rows will be removed first.
"""

# Duplicate rows control
duplicate_rows = df[df.duplicated()]
print(f"Number of duplicate rows: {len(duplicate_rows)}")

if len(duplicate_rows) > 0:
    print("Duplicate rows:")
    print(duplicate_rows)
else:
    print("No duplicate rows found.")

# Remove duplicates
df= df.drop_duplicates()
print(f"Number of rows after removing duplicates: {len(df)}")

"""<h4>Now the dataset has 150 rows and 93 columns.</h4>

Creation of a new dataset called df_new.
"""

df_new = df.copy()

"""Deletion of rows containing at least one null value (NaN) in the columns ranging from phq_1 to phq_9, as the final questionnaire score would otherwise be incomplete."""

df_new = df_new.dropna(subset=[f'phq_{i}' for i in range(1, 10)])

"""Deletion of rows containing at least one null value (NaN) in the columns ranging from gad_1 to gad_9, as the final questionnaire score would otherwise be incomplete."""

df_new = df_new.dropna(subset=[f'gad_{i}' for i in range(1, 8)])

"""Deletion of rows containing at least one null value (NaN) in the columns from ssba_1 to ssba_4, as the final questionnaire score would be incomplete."""

df_new = df_new.dropna(subset=[f'ssba_internet_{i}' for i in range(1, 5)])
df_new = df_new.dropna(subset=[f'ssba_drug_{i}' for i in range(1, 5)])
df_new = df_new.dropna(subset=[f'ssba_alcohol_{i}' for i in range(1, 5)])
df_new = df_new.dropna(subset=[f'ssba_gambling_{i}' for i in range(1, 5)])

df_new

"""<h4>A new dataset (df_new) has been obtained, consisting of 137 rows and 93 columns.<h4>

<h3> Processing columns <h3>

To analyze depression, the total score of the questionnaire will be calculated. Specifically, the scores from the columns phq_1 to phq_9 will be summed for each record. Subsequently, the columns phq_1 to phq_9 will be removed.
"""

df_new['phq_tot'] = df_new.loc[:, 'phq_1':'phq_9'].sum(axis=1)
df_new = df_new.drop(columns=[f'phq_{i}' for i in range(1, 10)])
df_new.head()

"""To analyze anxiety, the total score of the questionnaire will be calculated. Specifically, the scores from the columns gad_1 to gad_7 will be summed for each record. Subsequently, the columns gad_1 to gad_7 will be removed."""

df_new['gad_tot'] = df_new.loc[:, 'gad_1':'gad_7'].sum(axis=1)
df_new = df_new.drop(columns=[f'gad_{i}' for i in range(1, 8)])
df_new.head()

"""The focus is not on individuals with ADHD or ASD; therefore, the columns from asrs_1 to asrs_6 and from asq_1 to asq_50 will be removed."""

df_new = df_new.drop(columns=[f'asrs_{i}' for i in range(1, 7)])
df_new = df_new.drop(columns=[f'asq_{i}' for i in range(1, 51)])
df_new.head()

"""The focus will now shift to addictions."""

# The columns related to internet addiction will be summed to calculate a total score
df_new['ssba_internet_tot'] = df_new.loc[:, 'ssba_internet_1':'ssba_internet_4'].sum(axis=1)
df_new = df_new.drop(columns=[f'ssba_internet_{i}' for i in range(1, 5)])

# The columns related to drug addiction will be summed to calculate a total score
df_new['ssba_drug_tot'] = df_new.loc[:, 'ssba_drug_1':'ssba_drug_4'].sum(axis=1)
df_new = df_new.drop(columns=[f'ssba_drug_{i}' for i in range(1, 5)])

# The columns related to alcohol addiction will be summed to calculate a total score
df_new['ssba_alcohol_tot'] = df_new.loc[:, 'ssba_alcohol_1':'ssba_alcohol_4'].sum(axis=1)
df_new = df_new.drop(columns=[f'ssba_alcohol_{i}' for i in range(1, 5)])

# The columns related to gambling addiction will be summed to calculate a total score
df_new['ssba_gambling_tot'] = df_new.loc[:, 'ssba_gambling_1':'ssba_gambling_4'].sum(axis=1)
df_new = df_new.drop(columns=[f'ssba_gambling_{i}' for i in range(1, 5)])

df_new

"""<h4>A new dataset (df_new) has been obtained, consisting of 137 rows and 11 columns.<h4>

<h2>EXPLORATORY DATA ANALYSIS<h2>

<h3> Univariate Data Analysis <h3>

<h4>Shapiro-Wilk Test</h4>

The *Shapiro-Wilk test* is a hypothesis test that is applied to a sample with a null hypothesis that the sample has been generated from a normal distribution. If the p-value is low, we can reject such a null hypothesis and say that the sample has not been generated from a normal distribution.

This statistical tool is straightforward and effective for performing normality checks. However, it has a limitation: it does not perform well with large datasets. The maximum dataset size depends on the specific implementation, but in Python, sample sizes exceeding 5,000 provide only an approximate calculation of the p-value.

https://builtin.com/data-science/shapiro-wilk-test

shapiro(x, *, axis=None, nan_policy='propagate', keepdims=False)

Perform the Shapiro-Wilk test for normality.
The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution.

Parameters:

- x -> array_like: array of sample data. Must contain at least three observations.

- axis -> int or None, default: None. If an int, the axis of the input along which to compute the statistic. The statistic of each axis-slice (e.g. row) of the input will appear in a corresponding element of the output. If None, the input will be raveled before computing the statistic.

- nan_policy -> {‘propagate’, ‘omit’, ‘raise’}. Defines how to handle input NaNs. (propagate: if a NaN is present in the axis slice (e.g. row) along which the statistic is computed, the corresponding entry of the output will be NaN; omit: NaNs will be omitted when performing the calculation. If insufficient data remains in the axis slice along which the statistic is computed, the corresponding entry of the output will be NaN; raise: if a NaN is present, a ValueError will be raised).

- keepdims -> bool, default: False. If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.

Returns:

- statistic (float) -> The test statistic.

- p-value (float) -> The p-value for the hypothesis test.

https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html
"""

df_new.describe()

"""<h5>Age distribution</h5>

The age distribution will be analyzed.
"""

# Check for normality distribution
stat, p = shapiro(df_new['age'])
print(f"Statistics: {stat}")
print(f"P-value: {p}")

if p < 0.05:
    print("Data do not follow a normal distribution.")
else:
    print("Data follow a normal distribution.")

# Equal intervals will be created within the range of 18 to 80 (minimum and maximum values).
bins = np.linspace(18, 80, 8).astype(int)  # 7 intervals

# Histogram
plt.figure(figsize=(8, 6))
plt.hist(df_new['age'], bins=bins, edgecolor='black')
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Number of people')
plt.xticks(bins)
plt.grid(True)
plt.show()

"""In the dataset, most individuals fall within the age range of 26 to 35 years, followed by those aged 35 to 44 and 71 to 80 years.

<h5>Gender distribution</h5>
"""

labels = ['Male', 'Female', 'Non-binary', 'Prefer not to say']
gender_counts = df_new['gender'].value_counts().reindex([0, 1, 2, 3], fill_value=0)

# Bar chart
plt.figure(figsize=(8, 6))
plt.bar(labels, gender_counts, color='skyblue', edgecolor='black')
plt.title('Gender Distribution')
plt.xlabel('Gender')
plt.ylabel('Number of People')
plt.grid(True, axis='y')
plt.show()

"""The chart shows a slight majority of females, followed by males, and then individuals who prefer not to specify their gender.

<h5>Education distribution</h5>
"""

labels = ['Elementary school', 'Middle school', 'High School',
          "Bachelor's Degree", "Master's Degree", 'Doctoral Degree']
education_counts = df_new['education'].value_counts().reindex([5, 8, 13, 18, 22, 25], fill_value=0)

# Bar chart
plt.figure(figsize=(10, 6))
plt.bar(labels, education_counts, color='skyblue', edgecolor='black')
plt.title('Education Level Distribution')
plt.xlabel('Education Level')
plt.ylabel('Number of People')
plt.xticks(rotation=45)
plt.grid(True, axis='y')
plt.show()

"""The chart reveals a clear predominance of individuals with a high school diploma, followed by those with a bachelor's degree and those whose education ended at middle school.

<h5>Marital status distribution</h5>
"""

labels = ['Single', 'Married', 'Divorced', 'Widowed', 'Separated', 'Prefer not to say']
marital_counts = df_new['marital'].value_counts().reindex([0, 1, 2, 3, 4, 5], fill_value=0)

# Bar chart
plt.figure(figsize=(10, 6))
plt.bar(labels, marital_counts, color='skyblue', edgecolor='black')
plt.title('Marital Status Distribution')
plt.xlabel('Marital Status')
plt.ylabel('Number of People')
plt.grid(True, axis='y')
plt.show()

"""The chart displays a clear predominance of married individuals, followed by a smaller proportion of single individuals.

<h5>Income distribution</h5>
"""

income_min = df_new['income'].min()
income_max = df_new['income'].max()
bins = np.arange(income_min, income_max + 5000, 5000)

df_new = df_new.dropna(subset=['income'])

# Check for normality distribution
stat, p = shapiro(df_new['income'])
print(f"Statistics: {stat}")
print(f"P-value: {p}")
if p < 0.05:
    print("Data do not follow a normal distribution.")
else:
    print("Data follow a normal distribution.")

# Histrogram
plt.figure(figsize=(10, 6))
plt.hist(df_new['income'], bins=bins, color='skyblue', edgecolor='black')
plt.title('Income Distribution')
plt.xlabel('Income')
plt.ylabel('Number of People')
plt.gca().yaxis.set_major_locator(plt.MaxNLocator(integer=True))
plt.grid(True)
plt.show()

"""<h5>People distribution for depression (PHQ-9)</h5>"""

# Check for normality distribution
stat, p = shapiro(df_new['phq_tot'])
print(f"Statistics: {stat}")
print(f"P-value: {p}")
if p < 0.05:
    print("Data do not follow a normal distribution.")
else:
    print("Data follow a normal distribution.")

# The bin edges and corresponding labels for the intervals of the PHQ-9 depression scores are defined
bins = [0, 4, 9, 14, 19, 27]
labels = ['0-4\nNone-minimal', '5-9\nMild', '10-14\nModerate', '15-19\nModerately Severe', '20-27\nSevere']

# The number of individuals falling into each interval of PHQ-9 scores is counted
counts, _ = np.histogram(df_new['phq_tot'], bins=bins)

# Bar chart
plt.figure(figsize=(8, 6))
plt.bar(labels, counts, color='skyblue', edgecolor='black')
plt.title('People Distribution for Depression (PHQ-9)')
plt.xlabel('PHQ-9 points (Range of scores)')
plt.ylabel('Number of people')
plt.grid(True, axis='y')
plt.show()

"""The chart illustrates the distribution of individuals based on their PHQ-9 depression scores, which categorize the severity of depression into five levels:

- 0-4 (None-minimal): this is the second-largest category, with approximately 40 individuals showing little to no signs of depression;
- 5-9 (Mild): this represents the smallest group, with fewer than 10 individuals classified under mild depression;
- 10-14 (Moderate): around 35 individuals fall into this category, indicating symptoms of moderate depression;
- 15-19 (Moderately severe): the largest group, with over 45 individuals, shows moderately severe depression, highlighting a significant prevalence of serious depressive symptoms;
- 20-27 (Severe): slightly more than 15 individuals are categorized in the severe depression range, representing the group with the most critical symptoms.

<h5>People distribution for anxiety (GAD-7)</h5>
"""

# Check for normality distribution
stat, p = shapiro(df_new['gad_tot'])
print(f"Statistics: {stat}")
print(f"P-value: {p}")
if p < 0.05:
    print("Data do not follow a normal distribution.")
else:
    print("Data follow a normal distribution.")

# The bin edges and corresponding labels for the intervals are defined
bins = [0, 4, 7, 9, 14, 21]
labels = ['0-4\nNone-minimal', '5-7\nMild', '8-9\nMild', '10-14\nModerate', '15-21\nSevere']

# The number of individuals falling into each interval of GAD-7 scores is counted
counts, _ = np.histogram(df_new['phq_tot'], bins=bins)

# Bar chart
plt.figure(figsize=(8, 6))
plt.bar(labels, counts, color='skyblue', edgecolor='black')
plt.title('People Distribution for Anxiety (GAD-7)')
plt.xlabel('PHQ-9 points (Range of scores)')
plt.ylabel('Number of people')
plt.grid(True, axis='y')
plt.show()

"""This chart illustrates the distribution of individuals based on their GAD-7 anxiety scores, which assess the severity of generalized anxiety disorder (GAD). The scores are categorized into five levels of anxiety severity:
- 0-4 (None-minimal): this is the second-largest group, with approximately 40 individuals showing little to no symptoms of anxiety;
- 5-7 (Mild): a much smaller group, with fewer than 10 individuals, falls within this mild anxiety category;
- 8-9 (Mild): another small group, likely consisting of just one person, is categorized within this range;
- 10-14 (Moderate): around 20 individuals exhibit moderate anxiety symptoms, indicating notable levels of distress without reaching the severe threshold;
- 15-21 (Severe): the largest group, comprising over 60 individuals, falls into the severe anxiety category, highlighting a significant portion of the dataset experiencing high levels of anxiety according to the GAD-7 measure.


This distribution underscores the prevalence of severe anxiety within the dataset.

<h5>Addiction distribution</h5>
"""

# Check for normality distribution
stat, p = shapiro(df_new['ssba_internet_tot'])
print(f"Internet addiction statistics: {stat}")
print(f"Internet addiction p-value: {p}")
if p < 0.05:
    print("Data (internet addiction) do not follow a normal distribution.\n")
else:
    print("Data (internet addiction) follow a normal distribution.\n")

stat, p = shapiro(df_new['ssba_drug_tot'])
print(f"Drug addiction statistics: {stat}")
print(f"Drug addiction p-value: {p}")
if p < 0.05:
    print("Data (drug addiction) do not follow a normal distribution.\n")
else:
    print("Data (drug addiction) follow a normal distribution.\n")

stat, p = shapiro(df_new['ssba_alcohol_tot'])
print(f"Alcohol addiction statistics: {stat}")
print(f"Alcohol addiction p-value: {p}")
if p < 0.05:
    print("Data (alcohol addiction) do not follow a normal distribution.\n")
else:
    print("Data (alcohol addiction) follow a normal distribution.\n")

stat, p = shapiro(df_new['ssba_gambling_tot'])
print(f"Gambling addiction statistics: {stat}")
print(f"Gambling addiction p-value: {p}")
if p < 0.05:
    print("Data (gambling addiction) do not follow a normal distribution.\n")
else:
    print("Data (gambling addiction) follow a normal distribution.\n")

# The bin edges and labels for the addictions are defined
bins = [0, 3, 10, 16]
labels = ['0-3\nNone', '4-10\nPresent', '11-16\nSo much present']

# List of column names for each addiction type with their corresponding titles
addictions = {
    'Internet': 'ssba_internet_tot',
    'Drugs': 'ssba_drug_tot',
    'Alcohol': 'ssba_alcohol_tot',
    'Gambling': 'ssba_gambling_tot'
}

# Figure with 4 subplots
fig, axs = plt.subplots(2, 2, figsize=(12, 10))  # 2x2 grid of subplots

# Iteration through each addiction to create a separate bar chart
for ax, (addiction_name, column_name) in zip(axs.flatten(), addictions.items()):
    # The number of individuals falling into each interval for the specific addiction is counted
    counts, _ = np.histogram(df_new[column_name], bins=bins)

    # Bar chart for the current addiction
    ax.bar(labels, counts, color='skyblue', edgecolor='black')
    ax.set_title(f'{addiction_name} Dependency')
    ax.set_xlabel('Score Range')
    ax.set_ylabel('Number of People')
    ax.grid(True, axis='y')

plt.tight_layout()
plt.show()

"""<h3> Bivariate Analysis <h3>

<h4>Spearman's coefficient</h4>

spearmanr(a, b=None, axis=0, nan_policy='propagate', alternative='two-sided')

Calculate a Spearman correlation coefficient with associated p-value.

The *Spearman rank-order correlation coefficient* is a nonparametric measure of the monotonicity of the relationship between two datasets. Like other correlation coefficients, this one varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact monotonic relationship. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreases.

The p-value is a probability associated with the statistical test. Specifically, it represents the probability of observing a correlation value (in terms of strength and direction) at least as extreme as the one calculated from the data, assuming there is no correlation in the population (null hypothesis).

If the p-value is small (e.g., below a conventional threshold like 0.05), it indicates that it is highly unlikely to observe such a strong correlation (positive or negative) by chance in an uncorrelated system. This leads to rejecting the null hypothesis and concluding that there is likely a significant correlation between the two variables.

Parameters:
- a, b1D or 2D: array_like, b is optional

    One or two 1-D or 2-D arrays containing multiple variables and observations. When these are 1-D, each represents a vector of observations of a single variable. For the behavior in the 2-D case, see under axis, below. Both arrays need to have the same length in the axis dimension.

- axis: int or None, optional

    If axis=0 (default), then each column represents a variable, with observations in the rows. If axis=1, the relationship is transposed: each row represents a variable, while the columns contain observations. If axis=None, then both arrays will be raveled.

- nan_policy: {‘propagate’, ‘raise’, ‘omit’}, optional

    Defines how to handle when input contains nan. The following options are available (default is ‘propagate’):
        - ‘propagate’: returns nan
        - ‘raise’: throws an error
        - ‘omit’: performs the calculations ignoring nan values

- alternative: {‘two-sided’, ‘less’, ‘greater’}, optional

    Defines the alternative hypothesis. Default is ‘two-sided’. The following options are available:
        - ‘two-sided’: the correlation is nonzero
        - ‘less’: the correlation is negative (less than zero)
        - ‘greater’: the correlation is positive (greater than zero)

Returns:

resSignificanceResult

An object containing attributes:
- statistic: float or ndarray (2-D square)

    Spearman correlation matrix or correlation coefficient (if only 2 variables are given as parameters). Correlation matrix is square with length equal to total number of variables (columns or rows) in a and b combined.

- pvalue: float

    The p-value for a hypothesis test whose null hypothesis is that two samples have no ordinal correlation. See alternative above for alternative hypotheses. pvalue has the same shape as statistic.

https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html

<h4>Kruskall-Wallis Test</h4>

kruskal(*samples, nan_policy='propagate', axis=0, keepdims=False)

Compute the Kruskal-Wallis H-test for independent samples.

The Kruskal-Wallis H-test tests the null hypothesis that the population median of all of the groups are equal. The test works on 2 or more independent samples, which may have different sizes. Note that rejecting the null hypothesis does not indicate which of the groups differs.
It is used when the data do not meet the assumptions required for ANOVA (such as normality and/or equality of variances).

Parameters:
- sample1, sample2, ...: array_like

    Two or more arrays with the sample measurements can be given as arguments. Samples must be one-dimensional.
    
- nan_policy: {‘propagate’, ‘omit’, ‘raise’}

    Defines how to handle input NaNs.
        - propagate: if a NaN is present in the axis slice (e.g. row) along which the statistic is computed, the corresponding entry of the output will be NaN.
        - omit: NaNs will be omitted when performing the calculation. If insufficient data remains in the axis slice along which the statistic is computed, the corresponding entry of the output will be NaN.
        - raise: if a NaN is present, a ValueError will be raised.
        
- axis: int or None, default: 0

    If an int, the axis of the input along which to compute the statistic. The statistic of each axis-slice (e.g. row) of the input will appear in a corresponding element of the output. If None, the input will be raveled before computing the statistic.

- keepdims: bool, default: False

    If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.

Returns:
- statistic: float

    The Kruskal-Wallis H statistic, corrected for ties.
- pvalue: float

    The p-value for the test using the assumption that H has a chi square distribution. The p-value returned is the survival function of the chi square distribution evaluated at H.

https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html

<h5>Age</h5>
"""

# Scatter plot
plt.figure(figsize=(8, 6))
sns.scatterplot(x=df_new['phq_tot'], y=df_new['age'])
plt.title('Age in Function of PHQ-9 Depression Score')
plt.xlabel('PHQ-9 Depression Score')
plt.ylabel('Age')
plt.grid(True)
plt.show()

# Since none of the variables follow a normal distribution, the Spearman correlation will be used.
spearman_corr, spearman_p_value = spearmanr(df_new['phq_tot'], df_new['age'])
print(f"Spearman's coefficient: {spearman_corr:.3f}, p-value: {spearman_p_value:.3f}")

"""- Young People (20-35): there are individuals with both mild and severe levels of depression.
- Adults (30-50): many individuals seem to have lower depression scores (between 0 and 5).
- Elderly (60 and above): PHQ-9 scores tend to be higher, which might indicate a trend toward more severe levels of depression among older individuals.

The Spearman's coefficient between depression (PHQ-9 score) and age is -0.047. This indicates a very weak negative monotonic relationship between these two variables, meaning that as one variable increases, the other tends to slightly decrease, but the effect is extremely small. The p-value is 0.589, which is much higher than the common significance level of 0.05. So, the correlation is not statistically significant.

<h5>Gender</h5>
"""

# Box plot to compare depression scores with gender categories
plt.figure(figsize=(8, 6))
sns.boxplot(x=df_new['gender'], y=df_new['phq_tot'])
plt.xticks([0, 1, 2, 3], ['0\nMale', '1\nFemale', '2\nNon-binary', '3\nPrefer not to say'])
plt.title('PHQ-9 Depression vs Gender')
plt.xlabel('Gender')
plt.ylabel('PHQ-9 Depression Score')
plt.grid(True)
plt.show()

# Filtering groups with at least two valid values to ensure only groups with sufficient data are included in the test
valid_groups = [gender for gender in df_new['gender'].unique()
                if df_new[df_new['gender'] == gender]['phq_tot'].dropna().shape[0] > 1]

# Extracting PHQ-9 scores for valid groups
filtered_groups = [df_new[df_new['gender'] == gender]['phq_tot'].dropna() for gender in valid_groups]

# Perform the Kruskal-Wallis test on valid groups in order to compare the distribution of PHQ-9 scores across gender groups
if len(filtered_groups) > 1:
    stat, p_value = kruskal(*filtered_groups)
    print(f"Kruskal-Wallis Test: H-statistic = {stat:.3f}, p-value = {p_value:.3e}")
    if p_value < 0.05:
        print("There is a statistically significant difference in PHQ-9 scores between gender groups.")
    else:
        print("No statistically significant difference in PHQ-9 scores between gender groups.")
else:
    print("Not enough valid groups for Kruskal-Wallis Test.")  # Test can't be run

"""The Kruskal-Wallis test indicates a statistically significant difference in PHQ-9 depression scores across gender groups (H = 9.197, p = 0.0268). This suggests that at least one gender group differs significantly in its depression score distribution from the others.

<h5>Education</h5>
"""

# Box plot to compare depression scores with education categories
plt.figure(figsize=(8, 6))
sns.boxplot(x=df_new['education'], y=df_new['phq_tot'])

plt.xticks([0, 1, 2, 3, 4, 5], ['Elementary School', 'Middle School', 'High School', 'Bachelor\'s Degree', 'Master\'s Degree', 'Doctoral Degree'])

plt.title('PHQ-9 Depression vs Education')
plt.xlabel('Education')
plt.ylabel('PHQ-9 Depression Score')
plt.grid(True)
plt.show()

valid_groups = [education for education in df_new['education'].unique()
                if df_new[df_new['education'] == education]['phq_tot'].dropna().shape[0] > 1]
filtered_groups = [df_new[df_new['education'] == education]['phq_tot'].dropna() for education in valid_groups]
if len(filtered_groups) > 1:
    stat, p_value = kruskal(*filtered_groups)
    print(f"Kruskal-Wallis Test: H-statistic = {stat:.3f}, p-value = {p_value:.3e}")
    if p_value < 0.05:
        print("There is a statistically significant difference in PHQ-9 scores between education groups.")
    else:
        print("No statistically significant difference in PHQ-9 scores between education groups.")
else:
    print("Not enough valid groups for Kruskal-Wallis Test.")

"""The Kruskal-Wallis test shows a statistically significant difference in PHQ-9 depression scores across education levels (H = 29.906, p = 0.000015). This indicates that at least one education group has a significantly different depression score distribution compared to the others.

<h5>Marital status</h5>
"""

# Box plot
plt.figure(figsize=(8, 6))
sns.boxplot(x=df_new['marital'], y=df_new['phq_tot'])

plt.xticks([0, 1, 2, 3, 4, 5], ['Single', 'Married', 'Divorced', 'Widowed', 'Separated', 'Prefer not to say'])

plt.title('PHQ-9 Depression Score by Marital Status')
plt.xlabel('Marital Status')
plt.ylabel('PHQ-9 Depression Score')
plt.grid(True)
plt.show()

valid_groups = [marital for marital in df_new['marital'].unique()
                if df_new[df_new['marital'] == marital]['phq_tot'].dropna().shape[0] > 1]
filtered_groups = [df_new[df_new['marital'] == marital]['phq_tot'].dropna() for marital in valid_groups]
if len(filtered_groups) > 1:
    stat, p_value = kruskal(*filtered_groups)
    print(f"Kruskal-Wallis Test: H-statistic = {stat:.3f}, p-value = {p_value:.3e}")
    if p_value < 0.05:
        print("There is a statistically significant difference in PHQ-9 scores between marital status groups.")
    else:
        print("No statistically significant difference in PHQ-9 scores between marital status groups.")
else:
    print("Not enough valid groups for Kruskal-Wallis Test.")

"""The Kruskal-Wallis test indicates a statistically significant difference in PHQ-9 depression scores across marital status groups (H = 37.828, p = 4.085e-07). This suggests that at least one marital status group has a depression score distribution significantly different from the others.

<h5>Income</h5>
"""

# Scatter plot between PHQ-9 (depression) and income
plt.figure(figsize=(8, 6))
sns.scatterplot(x=df_new['phq_tot'], y=df_new['income'])
plt.title('Income in Function of PHQ-9 (Depression)')
plt.xlabel('PHQ-9 Depression Score')
plt.ylabel('Income')
plt.grid(True)
plt.show()

df_cleaned = df_new[['phq_tot', 'income']].dropna()
# Since we've checked that our datat is not normal we use the spearman test
spearman_corr_income, spearman_p_value = stats.spearmanr(df_new['phq_tot'], df_new['income'])
print(f"Spearman's coefficient: {spearman_corr_income:.3f}, p-value = {Decimal(spearman_p_value):.3e}")

"""The Spearman's coefficient between depression and income is -0.604, indicating a moderately strong negative monotonic relationship. This suggests that as income decreases, depression scores tend to increase. The p-value is extremely small, meaning this correlation is statistically significant.

<h5>Anxiety</h5>
"""

# Scatter plot
plt.figure(figsize=(8, 6))
sns.scatterplot(x=df_new['phq_tot'], y=df_new['gad_tot'])
plt.title('Scatter Plot of PHQ-9 (Depression) vs GAD-7 (Anxiety)')
plt.xlabel('PHQ-9 Depression Score')
plt.ylabel('GAD-7 Anxiety Score')
plt.grid(True)
plt.show()

spearman_corr_gad, spearman_p_value = spearmanr(df_new['phq_tot'], df_new['gad_tot'])
print(f"Spearman's coefficient: {spearman_corr_gad:.3f}, p-value = {Decimal(spearman_p_value):.2e}")

"""The Spearman's coefficient of 0.343 suggests a moderate positive monotonic relationship between depression and anxiety. The p-value indicates that this correlation is statistically significant, with strong evidence against the null hypothesis of no relationship.

<h5>Addictions</h5>
"""

# List of addictions and titles
addictions = ['ssba_internet_tot', 'ssba_drug_tot', 'ssba_alcohol_tot', 'ssba_gambling_tot']
addiction_titles = ['Internet Addiction', 'Drug Addiction', 'Alcohol Addiction', 'Gambling Addiction']

# Correlation results
correlation_results = {}

# Create scatter plot with PHQ-9 Depression on the x-axis
plt.figure(figsize=(18, 12))

for i, addiction in enumerate(addictions):
    plt.subplot(2, 2, i+1)
    sns.scatterplot(x=df_new['phq_tot'], y=df_new[addiction])
    plt.title(f'PHQ-9 Depression vs {addiction_titles[i]}')
    plt.xlabel('PHQ-9 Depression Score')
    plt.ylabel(f'{addiction_titles[i]} Score')

    # Calculate Pearson correlation
    spearman_corr, spearman_p_value = spearmanr(df_new['phq_tot'], df_new[addiction])
    correlation_results[addiction_titles[i]] = {
    'Method': 'Spearman',
    'Coefficient': spearman_corr,
    'p-value': Decimal(spearman_p_value)
}

plt.tight_layout()
plt.show()

for addiction, result in correlation_results.items():
    print(f"{addiction}: {result['Method']} Coefficient = {result['Coefficient']:.3f}, p-value = {result['p-value']:.2e}")

"""The results show statistically significant positive correlations between PHQ-9 (depression) and all types of addictions. Internet Addiction has the strongest correlation (Spearman's coefficient = 0.388, p-value = 3.13e-6), followed by Drug Addiction (0.377), Gambling Addiction (0.330), and Alcohol Addiction (0.304). These findings suggest that higher depression scores are moderately associated with higher addiction scores.

<h3>Multivariate Analysis<h3>
"""

sns.heatmap(df_new.corr(), annot=True, cmap='coolwarm')

"""<h2>DATA ANALYSIS<h2>

<h3>Data Preparation<h3>
"""

df.info()

df_clusters = df.copy()

# Rows with null values in the required columns will be dropped
df_clusters = df_clusters.dropna(subset=['gender', 'marital', 'income'])
df_clusters = df_clusters.dropna(subset=[f'phq_{i}' for i in range(1, 10)])
df_clusters = df_clusters.dropna(subset=[f'gad_{i}' for i in range(1, 8)])
df_clusters = df_clusters.dropna(subset=[f'ssba_internet_{i}' for i in range(1, 5)])
df_clusters = df_clusters.dropna(subset=[f'ssba_drug_{i}' for i in range(1, 5)])
df_clusters = df_clusters.dropna(subset=[f'ssba_alcohol_{i}' for i in range(1, 5)])
df_clusters = df_clusters.dropna(subset=[f'ssba_gambling_{i}' for i in range(1, 5)])

# Columns that are not relevant for the analysis will be removed
df_clusters = df_clusters.drop(columns=[f'asrs_{i}' for i in range(1, 7)])
df_clusters = df_clusters.drop(columns=[f'asq_{i}' for i in range(1, 51)])

# The columns containing PHQ scores will be summed
df_clusters['phq_tot'] = df_clusters.loc[:, 'phq_1':'phq_9'].sum(axis=1)
df_clusters = df_clusters.drop(columns=[f'phq_{i}' for i in range(1, 10)])

# The columns containing GAD scores will be summed
df_clusters['gad_tot'] = df_clusters.loc[:, 'gad_1':'gad_7'].sum(axis=1)
df_clusters = df_clusters.drop(columns=[f'gad_{i}' for i in range(1, 8)])

# The columns containing addiction scores will be summed
df_clusters['ssba_internet_tot'] = df_clusters.loc[:, 'ssba_internet_1':'ssba_internet_4'].sum(axis=1)
df_clusters = df_clusters.drop(columns=[f'ssba_internet_{i}' for i in range(1, 5)])

df_clusters['ssba_drug_tot'] = df_clusters.loc[:, 'ssba_drug_1':'ssba_drug_4'].sum(axis=1)
df_clusters = df_clusters.drop(columns=[f'ssba_drug_{i}' for i in range(1, 5)])

df_clusters['ssba_alcohol_tot'] = df_clusters.loc[:, 'ssba_alcohol_1':'ssba_alcohol_4'].sum(axis=1)
df_clusters = df_clusters.drop(columns=[f'ssba_alcohol_{i}' for i in range(1, 5)])

df_clusters['ssba_gambling_tot'] = df_clusters.loc[:, 'ssba_gambling_1':'ssba_gambling_4'].sum(axis=1)
df_clusters = df_clusters.drop(columns=[f'ssba_gambling_{i}' for i in range(1, 5)])

# The index must be from 0 to the maximum number of the actual dataset df_clusters
df_clusters.reset_index(drop=True, inplace=True)

df_clusters

"""To include categorical variables in Hierarchical clustering, I need to convert them into numerical variables. I decide to use the One-Hot Encoding, which transforms categorical variables into binary variables. I do this for the variables 'gender' and 'marital'.


Education is a categorical variable that represents an ordered progression so, I don't  apply One-Hot Encoding. I treat them as ordinal variables. With Hierarchical clustering, the variable is treated as continuous data. The same is for the phq responses and the gad responses and the ssba responses.

<h3>One-Hot Encoding</h3>
One-hot encoding is a technique for representing categorical data as numerical vectors, where each unique category is represented by a binary column with a value of 1 indicating its presence and 0 indicating its absence.


https://www.datacamp.com/tutorial/one-hot-encoding-python-tutorial

pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None)
Convert categorical variable into dummy/indicator variables.

Each variable is converted in as many 0/1 variables as there are different values. Columns in the output are each named after a value; if the input is a DataFrame, the name of the original variable is prepended to the value.

https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html
"""

df_clusters['gender'] = df_clusters['gender'].astype(int)
df_clusters['marital'] = df_clusters['marital'].astype(int)
df_clusters['education'] = df_clusters['education'].astype(int)

# One-Hot Encoding
df_clusters = pd.get_dummies(df_clusters, columns=['gender', 'marital', 'education'], drop_first=False)

df_clusters

"""<h3>Hierarchical clustering<h3>"""

# The data are standardized to have a mean of 0 and a variance of 1
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_clusters)

linkage_matrix = linkage(scaled_data, method='ward')

# Plot
plt.figure(figsize=(12, 8))
dendrogram(linkage_matrix, truncate_mode='lastp', p=30, leaf_rotation=90., leaf_font_size=12., show_contracted=True)
plt.title('Dendrogram for Hierarchical Clustering')
plt.xlabel('Data Points')
plt.ylabel('Distance')
plt.axhline(y=150, color='r', linestyle='--')
plt.show()

"""The graph indicates that 3 is the optimal number of clusters.

<h3>Optimal Number of Clusters<h3>
"""

num_clusters = 3

# Assign cluster labels to each data point
cluster_labels = fcluster(linkage_matrix, t=num_clusters, criterion='maxclust')

# Add the cluster labels to the original dataframe
df_clusters['Cluster'] = cluster_labels

# Display cluster distribution
print("Cluster Distribution:\n", df_clusters['Cluster'].value_counts())

# Visualize clusters using the first two features as an example
plt.figure(figsize=(10, 6))
sns.scatterplot(x=scaled_data[:, 0], y=scaled_data[:, 1], hue=cluster_labels, palette='Set2', s=50)
plt.title('Clusters Visualized with Original Features')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend(title='Cluster', loc='best')
plt.grid(True)
plt.savefig("results_Cluster.png")
plt.show()

# Calculate silhouette score for the clustering
silhouette_avg = silhouette_score(scaled_data, cluster_labels)
print(f"Silhouette Score for Hierarchical Clustering: {silhouette_avg}")

# Compute silhouette scores for each sample
sample_silhouette_values = silhouette_samples(scaled_data, cluster_labels)

# Plot silhouette analysis
plt.figure(figsize=(10, 6))
y_lower = 10  # Starting point for the silhouette plot

for i in range(1, num_clusters + 1):
    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
    ith_cluster_silhouette_values.sort()
    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i

    plt.fill_betweenx(np.arange(y_lower, y_upper),
                      0, ith_cluster_silhouette_values, alpha=0.7)
    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
    y_lower = y_upper + 10  # Add space between clusters

plt.axvline(x=silhouette_avg, color="red", linestyle="--")
plt.title("The silhouette plot for the various clusters")
plt.xlabel("The silhouette coefficient values")
plt.ylabel("Cluster label")
plt.yticks([])
plt.savefig("results_silhouette.png")
plt.show()

"""<h3>Clusters<h3>"""

print(df_clusters.columns)

df_clusters.info()

"""I divide the dataset according to the three clusters."""

# Division of the dataset in three datasets based on the three clusters
cluster_1 = df_clusters[df_clusters['Cluster'] == 1]
cluster_2 = df_clusters[df_clusters['Cluster'] == 2]
cluster_3 = df_clusters[df_clusters['Cluster'] == 3]

cluster_1

cluster_2

cluster_3

"""<h3>Statistical Analysis</h3>

<h5>Nominal variables</h5>
"""

n_cluster = 3
alpha_corrected = 0.05/n_cluster

# Prepare list to store p-values
p_values = []
p_index = 0

# Function to evaluate p-values
def different(p_value, alpha):
    if p_value < alpha:
        print(f"There are differences: p_value {p_value} < alpha {alpha}\n")
    else:
        print(f"There are no differences: p_value {p_value} > alpha {alpha}\n")


df_clusters['gender'] = df_clusters[['gender_0', 'gender_1', 'gender_2', 'gender_3']].idxmax(axis=1)

# Create contingency table for gender vs. Cluster
contingency_table_gender = pd.crosstab(df_clusters['gender'], df_clusters['Cluster'])
print("Contingency Table (Gender vs. Cluster):\n", contingency_table_gender)

# Perform Chi-squared test
chi2, p, dof, expected = chi2_contingency(contingency_table_gender)

# Check if all expected frequencies are >= 5
if (expected < 5).any():
    print("Some expected frequencies are LESS THAN 5. Chi-squared test may not be valid.\n")
else:
    print("All expected frequencies are >= 5. Chi-squared test is valid.\n")

# Append p-value and evaluate significance
p_values.append(p)
print(f"\nChi-squared: {chi2}, p-value: {p_values[p_index]}\n")
different(p_values[p_index], 0.05)

# Compare only Cluster 0 and Cluster 1
contingency_subtable_gender_0_1 = contingency_table_gender.iloc[:, [0, 1]]
print("Contingency Table (Gender vs. Clusters 0 and 1):\n", contingency_subtable_gender_0_1)
p_index += 1
chi2, p, dof, expected = chi2_contingency(contingency_subtable_gender_0_1)
p_values.append(p)
print(f"\nChi-squared: {chi2}, p-value: {p_values[p_index]}\n")
different(p_values[p_index], 0.05)

# Compare only Cluster 0 and Cluster 2
contingency_subtable_gender_0_2 = contingency_table_gender.iloc[:, [0, 2]]
print("Contingency Table (Gender vs. Clusters 0 and 2):\n", contingency_subtable_gender_0_2)
p_index += 1
chi2, p, dof, expected = chi2_contingency(contingency_subtable_gender_0_2)
p_values.append(p)
print(f"\nChi-squared: {chi2}, p-value: {p_values[p_index]}\n")
different(p_values[p_index], alpha_corrected)

# Compare only Cluster 1 and Cluster 2
contingency_subtable_gender_1_2 = contingency_table_gender.iloc[:, [1, 2]]
print("Contingency Table (Gender vs. Clusters 1 and 2):\n", contingency_subtable_gender_1_2)
p_index += 1
chi2, p, dof, expected = chi2_contingency(contingency_subtable_gender_1_2)
p_values.append(p)
print(f"\nChi-squared: {chi2}, p-value: {p_values[p_index]}\n")
different(p_values[p_index], alpha_corrected)

"""<h5>Numerical Variables</h5>

 *Kruskal-Wallis test* is used to compare the distributions of three or more independent groups. It is an extension of the *Mann-Whitney U test*, which is used to compare two groups.

* kruskal(*samples, nan_policy='propagate', axis=0, keepdims=False)
    
  Compute the Kruskal-Wallis H-test for independent samples.
  
  The Kruskal-Wallis H-test tests the null hypothesis that the population median of all of the groups are equal. The test works on 2 or more independent samples, which may have different sizes. Note that rejecting the null hypothesis does not indicate which of the groups differs. Post hoc comparisons between groups are required to determine which groups are different.
  
    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html

* mannwhitneyu(x, y, use_continuity=True, alternative='two-sided', axis=0, method='auto', *, nan_policy='propagate', keepdims=False)

    Perform the Mann-Whitney U rank test on two independent samples.

    The Mann-Whitney U test is a nonparametric test of the null hypothesis that the distribution underlying sample x is the same as the distribution underlying sample y. It is often used as a test of difference in location between distributions.
  
    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html
"""

# Map education levels to ordinal values
education_map = {
    'education_5': 'Elementary school',
    'education_8': 'Middle school',
    'education_13': 'High School',
    'education_18': "Bachelor's Degree",
    'education_22': "Master's Degree",
    'education_25': 'Doctoral Degree'
}

education_ordinal_map = {
    'Elementary school': 1,
    'Middle school': 2,
    'High School': 3,
    "Bachelor's Degree": 4,
    "Master's Degree": 5,
    'Doctoral Degree': 6
}

# Create 'education_level' and 'education_level_ordinal'
df_clusters['education_level'] = None
for column, level in education_map.items():
    df_clusters.loc[df_clusters[column] == 1, 'education_level'] = level
df_clusters['education_level_ordinal'] = df_clusters['education_level'].map(education_ordinal_map)

# Map education levels to ordinal values
education_map = {
    'marital_0': 'Single',
    'marital_1': 'Married',
    'marital_2': 'Divorced',
    'marital_3': 'Widowed',
    'marital_4': 'Separated',
    'marital_5': 'Prefer not to say'
}

education_ordinal_map = {
    'Single': 1,
    'Married': 2,
    'Divorced': 3,
    'Widowed': 4,
    'Separated': 5,
    'Prefer not to say': 6
}

# Create 'education_level' and 'education_level_ordinal'
df_clusters['maritial_level'] = None
for column, level in education_map.items():
    df_clusters.loc[df_clusters[column] == 1, 'maritial_level'] = level
df_clusters['maritial_level_ordinal'] = df_clusters['maritial_level'].map(education_ordinal_map)

# Define ordinal variables
ordinal = ['age', 'income', 'maritial_level_ordinal','education_level_ordinal', 'phq_tot', 'gad_tot',
           'ssba_internet_tot', 'ssba_drug_tot', 'ssba_alcohol_tot', 'ssba_gambling_tot']

# Statistical test parameters
n_cluster = 3
alpha_corrected = 0.05 / n_cluster
p_values = []
p_index = 0

# Function to check and print whether there are significant differences
def different(p_value, alpha):
    if p_value < alpha:
        print(f'There are differences (p-value: {p_value:.4e} < alpha: {alpha:.4e})')
    else:
        print(f'There are no differences (p-value: {p_value:.4e} >= alpha: {alpha:.4e})')

# Analysis loop
for col in ordinal:
    print(f'\nAnalyzing {col}:')

    # Extract data for each cluster
    cluster_3 = df_clusters[df_clusters['Cluster'] == 3][col]
    cluster_1 = df_clusters[df_clusters['Cluster'] == 1][col]
    cluster_2 = df_clusters[df_clusters['Cluster'] == 2][col]

    # Calculate and print statistics
    for cluster, label in zip([cluster_3, cluster_1, cluster_2], ['Cluster 3', 'Cluster 1', 'Cluster 2']):
        median = round(statistics.median(cluster))
        q1 = np.percentile(cluster, 25)
        q3 = np.percentile(cluster, 75)
        print(f'{label} -> Median: {median}, 1st percentile: {q1}, 3rd percentile: {q3}')

    # Kruskal-Wallis test
    print('\nKruskal-Wallis Test:')
    stat, p = kruskal(cluster_3, cluster_1, cluster_2)
    p_values.append(p)
    different(p_values[p_index], 0.05)
    p_index += 1

    # Mann-Whitney U tests
    print('\nMann-Whitney U Tests:')
    for (c1, c2, label) in [(cluster_3, cluster_1, '3-1'),
                            (cluster_3, cluster_2, '3-2'),
                            (cluster_1, cluster_2, '1-2')]:
        stat, p = mannwhitneyu(c1, c2, alternative='two-sided')
        print(f'Cluster {label}:')
        different(p, alpha_corrected)
